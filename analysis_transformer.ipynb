{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac12fe8e",
   "metadata": {},
   "source": [
    "# Transformer-based discovery of neuromodulatory co-expression modules\n",
    "\n",
    "This is a preliminary experiment to utilize the *attentional* component of transformer-based deep learning models to identify sets of neuromodulator-related genes that tend to be expressed in some contexts but not in others.\n",
    "\n",
    "Transformers, though *overkill*, are very good at learning higher-order interactions, especially context. The statement \"Gene B predicts Gene A but only near vasculature\" is something a transformer is good at learning. The output of this model is which genes *attend* to others across cells, creating gene-gene dependencies (\"attention maps\"). These are latent modules that are **conditional** on *space*, *region*, or other hidden cell states.\n",
    "\n",
    "Data:\n",
    "- ~200D vector per cell, + spatial coordinates & metadata (cell type, brain region)\n",
    "- Each *gene* is a token\n",
    "- *Positional encoding* based on spatial location\n",
    "\n",
    "### Model spec\n",
    "token_input = gene_id_embedding + f(expression_value) + cell_context_embedding, where the former is a learnable embedding of gene identity and the latter is a linear transformation of the expression value\n",
    "\n",
    "### Architecture\n",
    "Stack of transformer encoder blocks, where each block has a multi-head self-attention + feed-forward layer. Output -- one embedding per gene or cell-level aggregated embedding\n",
    "\n",
    "### Learning\n",
    "**Contrastive learning** encourages cells to learn from context, compressing them into a low-dimensional embedding space where contextual similarity is preserved. This encourages learning features of the data that discriminate spatial context.\n",
    "\n",
    "Help to learn cell-level representations\n",
    "\n",
    "### Interpretability\n",
    "Transformers provide *attention scores* which tell you something like, \"In this cell, gene A attends most to gene B and C.\" This can be used to identify\n",
    "context-specific gene-gene depencies, cluster genes into modules based on shared attention, or to visualize maps across space. We can also *cluster* to define\n",
    "co-expression modules.\n",
    "\n",
    "### Loss\n",
    "1. Masked gene modelling -- mask random 15% of genes and predict expression with MSE or Huber loss\n",
    "`L_mask = mean_squared_error(predicted, true_expression)`\n",
    "2. Contrastive loss -- InfoNCE between pairs of cells (negative pairs meaning random cells from distanct regions, or positive from same spatial bin or region)\n",
    "`L_contrastive = -log( exp(sim(z1, z2) / τ) / sum(exp(sim(z1, zk) / τ)) )`\n",
    "\n",
    "$L_{total} = \\lambda_{mask} L_{mask} + \\lambda_{contrastive} L_{contrastive}$\n",
    "\n",
    "### Additional tests\n",
    "- Use attention scores to see which genes are consistently “important” in which contexts\n",
    "- Perturb gene inputs (e.g., set one to zero) and see how predictions or attention change\n",
    "- Cluster cell embeddings to find distinct neuromodulatory states or microcircuit motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80070362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_gene_modules.py\n",
    "# Basic transformer with two heads -- masked gene prediction and contrastive learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import allendata\n",
    "import gc # memory handling\n",
    "\n",
    "class AttnTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, src, src_mask=None, is_causal=False, src_key_padding_mask=None):\n",
    "        src2, attn = self.self_attn(\n",
    "            src, src, src,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False,\n",
    "            is_causal=is_causal  # <- new\n",
    "        )\n",
    "        self.attn_weights = attn.detach()\n",
    "        return super().forward(\n",
    "            src, src_mask=src_mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "class GeneTransformer(nn.Module):\n",
    "    # Constructor and hyperparameters\n",
    "    # num_genes: number of unique genes in the dataset\n",
    "    # d_model: dimensionality of the embeddings\n",
    "    # nhead: number of attention heads\n",
    "    # num_layers: number of transformer encoder layers\n",
    "    # dim_feedforward: dimensionality of the feedforward network\n",
    "    # dropout: dropout rate for regularization\n",
    "    def __init__(self, \n",
    "                 num_genes=200,\n",
    "                 d_model=128,\n",
    "                 nhead=8,\n",
    "                 num_layers=4,\n",
    "                 dim_feedforward=256,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings -- each gene gets a learnable embedding vector of size d\n",
    "        self.gene_embedding = nn.Embedding(num_genes, d_model)\n",
    "        self.expr_embedding = nn.Linear(1, d_model) # Projects scalar gene expression value into same d_model space\n",
    "\n",
    "        # Positional encoding (optional, if using gene order)\n",
    "        #self.pos_embedding = nn.Parameter(torch.randn(1, num_genes, d_model))\n",
    "\n",
    "        # Transformer encoder first define the layer\n",
    "        encoder_layer = AttnTransformerEncoderLayer(\n",
    "            d_model=d_model * 2,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Then create the encoder itself\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Masked gene prediction head (token-level)\n",
    "        self.mask_head = nn.Linear(d_model * 2, 1) # takes in your vector and outputs a scalar for each gene\n",
    "\n",
    "        # Contrastive learning head (cell-level)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1) # pools per-gene outputs to create a single vector per cell\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model * 2)\n",
    "        ) # Non-linear projection for contrastive learning (SimCLR)\n",
    "\n",
    "    def forward(self, gene_ids, expr, mask=None, return_attention=False):\n",
    "        # Where B = batch size, T = number of genes, D = embedding dimension\n",
    "        # gene_ids: (B, T)\n",
    "        # expr: (B, T, 1)\n",
    "        x_id = self.gene_embedding(gene_ids)           # (B, T, D)\n",
    "        x_expr = self.expr_embedding(expr)             # (B, T, D)\n",
    "        # concatenate the embeddings instead of summing\n",
    "        x = torch.cat([x_id, x_expr], dim=-1)  # (B, T, 2D)\n",
    "\n",
    "        # Capture attention weights by running layers manually\n",
    "        if return_attention:\n",
    "            # Only get attention from the first layer for simplicity\n",
    "            layer = self.transformer.layers[0]\n",
    "            attn_module = layer.self_attn\n",
    "            attn_output, attn_weights = attn_module(x, x, x, need_weights=True, average_attn_weights=False)\n",
    "            return attn_weights  # shape: (B, num_heads, T, T)\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=mask)           # (B, T, 2D)\n",
    "\n",
    "        # Masked prediction output\n",
    "        mask_pred = self.mask_head(x).squeeze(-1)      # (B, T)\n",
    "\n",
    "        # Cell-level embedding for contrastive learning\n",
    "        pooled = self.pooler(x.transpose(1, 2)).squeeze(-1)  # (B, T, D) -> (B, D)\n",
    "        proj = self.projection_head(pooled)            # (B, D)\n",
    "\n",
    "        return mask_pred, proj\n",
    "\n",
    "\n",
    "def mask_gene_expressions(expr, mask_ratio=0.15):\n",
    "    \"\"\"\n",
    "    expr: Tensor of shape (B, T, 1)\n",
    "    Returns:\n",
    "        masked_expr: same shape, with some tokens zeroed\n",
    "        mask: Bool tensor of shape (B, T) where True = masked\n",
    "    \"\"\"\n",
    "    B, T, _ = expr.shape\n",
    "    mask = torch.rand(B, T) < mask_ratio\n",
    "    masked_expr = expr.clone()\n",
    "    masked_expr[mask.unsqueeze(-1)] = 0.0  # or NaN, or learned <MASK> token\n",
    "\n",
    "    return masked_expr, mask\n",
    "\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    Compute MSE only on the masked gene positions.\n",
    "    \n",
    "    Args:\n",
    "        pred:   (B, T) model predictions\n",
    "        target: (B, T) ground truth expression values\n",
    "        mask:   (B, T) boolean tensor (True where loss should be computed)\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    masked_pred = pred[mask]\n",
    "    masked_target = target[mask]\n",
    "    return F.mse_loss(masked_pred, masked_target)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(embeddings, pos_mask, temperature=0.1):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor of shape (B, D)\n",
    "    pos_mask:   Bool tensor of shape (B, B) where pos_mask[i, j] = True if i and j are a positive pair\n",
    "    temperature: Scaling factor for similarity\n",
    "\n",
    "    Returns:\n",
    "        scalar contrastive loss (averaged over positive pairs)\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = F.normalize(embeddings, dim=1)  # (B, D)\n",
    "    sim_matrix = torch.matmul(embeddings, embeddings.T)  # (B, B) ; cosine similarity\n",
    "    sim_matrix = sim_matrix / temperature  # Scale by temperature\n",
    "    self_mask = torch.eye(sim_matrix.size(0), dtype=torch.bool, device=embeddings.device) # Mask self-similarity\n",
    "    sim_matrix = sim_matrix.masked_fill(self_mask, float('-inf'))\n",
    "    log_probs = F.log_softmax(sim_matrix, dim=1) # Log softmax to get log probabilities\n",
    "    loss = -log_probs[pos_mask].mean() # Average over positive pairs\n",
    "    return loss\n",
    "\n",
    "def make_spatial_pos_mask(coords: torch.Tensor, distance_threshold: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Constructs a (B, B) mask where [i, j] = True if Euclidean distance between\n",
    "    cell i and j is less than `distance_threshold`.\n",
    "\n",
    "    Args:\n",
    "        coords: Tensor of shape (B, 3), representing x, y, z positions of each cell\n",
    "        distance_threshold: scalar (e.g., 50.0)\n",
    "\n",
    "    Returns:\n",
    "        pos_mask: Boolean tensor of shape (B, B)\n",
    "    \"\"\"\n",
    "    B = coords.size(0)\n",
    "    diff = coords.unsqueeze(1) - coords.unsqueeze(0)  # (B, B, 3)\n",
    "    dists = torch.norm(diff, dim=-1)                  # (B, B)\n",
    "\n",
    "    not_self = ~torch.eye(B, dtype=torch.bool, device=coords.device)\n",
    "\n",
    "    pos_mask = (dists < distance_threshold) & not_self\n",
    "    return pos_mask\n",
    "\n",
    "def plot_embeddings(embeddings, title=\"Cell Embeddings (PCA)\"):\n",
    "    \"\"\"\n",
    "    embeddings: (N, D) tensor or numpy array of cell embeddings\n",
    "    \"\"\"\n",
    "    embeddings_np = embeddings.detach().cpu().numpy() if isinstance(embeddings, torch.Tensor) else embeddings\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_umap(embeddings, title=\"Cell Embeddings (UMAP)\"):\n",
    "    # Reduce to 2D using UMAP\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1)\n",
    "    reduced = reducer.fit_transform(embeddings.detach().cpu().numpy())\n",
    "\n",
    "    # Plot the result\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP 1\")\n",
    "    plt.ylabel(\"UMAP 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa6ee6",
   "metadata": {},
   "source": [
    "We might want to learn the threshold or set a soft loss based on distance\n",
    "\t•\tTry adding a soft loss based on distance\n",
    "\t•\tOr a learned position embedding to see if it gives you richer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c061254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "goi = 'Drd|Chrn|Chrm|Htr|Adra|Adrb|Gri|Grm|Gab|Opr|Sigmar|Cnr|Glr|Hrh|Slc6a2|Slc6a4|Slc6a3'\n",
    "expression_full, gene, _, _, _ = allendata.load_preprocessed()\n",
    "expression_full = expression_full[expression_full['dataset'] == 'MERFISH']\n",
    "expression_full = expression_full.dropna(subset=['x_reconstructed', 'y_reconstructed', 'z_reconstructed'])\n",
    "\n",
    "# Separate data\n",
    "genelist = gene['gene_symbol'].tolist()\n",
    "expression = expression_full[genelist]\n",
    "metadata = expression_full[expression_full.columns.difference(genelist)]\n",
    "expression = expression.fillna(0)\n",
    "\n",
    "# Filter, subset, convert to tensor\n",
    "num_cells = 1000\n",
    "sample_idx = torch.randint(0, expression.shape[0], (num_cells,))\n",
    "expression = expression.iloc[sample_idx]\n",
    "expr = torch.tensor(expression.values, dtype=torch.float32)\n",
    "expr = expr.unsqueeze(-1)\n",
    "metadata = metadata.iloc[sample_idx]\n",
    "coords = metadata[['x_reconstructed','y_reconstructed','z_reconstructed']]\n",
    "coords = torch.tensor(coords.values, dtype=torch.float32)\n",
    "\n",
    "gene_ids = torch.arange(expression.shape[1]).unsqueeze(0).repeat(num_cells, 1)\n",
    "\n",
    "del goi, genelist, expression_full, metadata, expression\n",
    "gc.collect()\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "expr = expr.to(device)\n",
    "gene_ids = gene_ids.to(device)\n",
    "coords = coords.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "num_genes = expr.shape[1]\n",
    "d_model = 128\n",
    "batch_size = 512\n",
    "num_epochs = 5\n",
    "l_contrastive = 1.0\n",
    "l_mask = 0\n",
    "distance_threshold = 1\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = GeneTransformer(num_genes=num_genes, d_model=d_model).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss tracking\n",
    "loss_history = {\"total\": [], \"masked\": [], \"contrastive\": []}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    perm = torch.randperm(num_cells)\n",
    "    epoch_mloss = 0.0\n",
    "    epoch_closs = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for i in tqdm(range(0, num_cells, batch_size)):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        batch_gene_ids = gene_ids[idx]\n",
    "        batch_expr = expr[idx]\n",
    "        batch_coords = coords[idx]\n",
    "\n",
    "        masked_expr, mask = mask_gene_expressions(batch_expr, mask_ratio=0.15)\n",
    "        mask_pred, cell_embedding = model(batch_gene_ids, masked_expr)\n",
    "\n",
    "        mloss = masked_mse_loss(mask_pred, batch_expr.squeeze(-1), mask)\n",
    "        pos_mask = make_spatial_pos_mask(batch_coords, distance_threshold)\n",
    "        closs = contrastive_loss(cell_embedding, pos_mask)\n",
    "\n",
    "        loss = l_mask * mloss + l_contrastive * closs\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "        epoch_mloss += mloss.item()\n",
    "        epoch_closs += closs.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Record losses\n",
    "    mean_mloss = epoch_mloss / num_batches\n",
    "    mean_closs = epoch_closs / num_batches\n",
    "    mean_total = l_mask * mean_mloss + l_contrastive * mean_closs\n",
    "\n",
    "    loss_history[\"masked\"].append(mean_mloss)\n",
    "    loss_history[\"contrastive\"].append(mean_closs)\n",
    "    loss_history[\"total\"].append(mean_total)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"runs/gene_transformer_epoch{epoch+1}.pt\")\n",
    "\n",
    "    # Visualize embeddings\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_idx = torch.randint(0, num_cells, (1000,))\n",
    "        sample_gene_ids = gene_ids[sample_idx]\n",
    "        sample_expr = expr[sample_idx]\n",
    "        masked_expr, _ = mask_gene_expressions(sample_expr)\n",
    "        _, sample_embeddings = model(sample_gene_ids, masked_expr)\n",
    "        plot_embeddings(sample_embeddings, title=f\"Epoch {epoch+1} Cell Embeddings\")\n",
    "\n",
    "    # Record scalar losses\n",
    "    writer.add_scalar(\"Loss/Total\", mean_total, epoch)\n",
    "    writer.add_scalar(\"Loss/Masked\", mean_mloss, epoch)\n",
    "    writer.add_scalar(\"Loss/Contrastive\", mean_closs, epoch)\n",
    "\n",
    "    # Log sample embeddings (every N epochs)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        writer.add_embedding(\n",
    "            sample_embeddings,\n",
    "            tag=f\"Epoch_{epoch+1}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Masked Loss: {mean_mloss:.4f} | Contrastive Loss: {mean_closs:.4f} | Total: {mean_total:.4f}\")\n",
    "\n",
    "writer.flush()\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history[\"total\"], label=\"Total Loss\")\n",
    "plt.plot(loss_history[\"masked\"], label=\"Masked MSE\")\n",
    "plt.plot(loss_history[\"contrastive\"], label=\"Contrastive\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot UMAP\n",
    "# After model evaluation (for example, after getting sample_embeddings)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    sample_idx = torch.randint(0, num_cells, (1000,))\n",
    "    sample_gene_ids = gene_ids[sample_idx]\n",
    "    sample_expr = expr[sample_idx]\n",
    "    masked_expr, _ = mask_gene_expressions(sample_expr)\n",
    "    _, sample_embeddings = model(sample_gene_ids, masked_expr)\n",
    "    \n",
    "    # Visualize with UMAP\n",
    "    plot_umap(sample_embeddings, title=f\"Epoch {epoch+1} Cell Embeddings (UMAP)\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_attention(model, gene_ids, expr, return_tensor=True):\n",
    "    \"\"\"\n",
    "    Extracts attention maps from all layers and heads.\n",
    "    Args:\n",
    "        model: Trained GeneTransformer\n",
    "        gene_ids: (B, T) tensor\n",
    "        expr: (B, T, 1) tensor\n",
    "        return_tensor: If True, returns a tensor (L, B, H, T, T), else returns list of Tensors\n",
    "    Returns:\n",
    "        Tensor of shape (L, B, H, T, T)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(gene_ids, expr)\n",
    "\n",
    "    # Collect attention from each layer\n",
    "    attn_all = []\n",
    "    for layer in model.transformer.layers:\n",
    "        attn = layer.attn_weights  # (B, H, T, T)\n",
    "        attn_all.append(attn.unsqueeze(0))  # (1, B, H, T, T)\n",
    "\n",
    "    if return_tensor:\n",
    "        return torch.cat(attn_all, dim=0)  # (L, B, H, T, T)\n",
    "    else:\n",
    "        return attn_all\n",
    "    \n",
    "def build_multilayer_gene_network(attn_tensor, gene_names, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Create a gene attention network from (L, B, H, T, T) attention tensor.\n",
    "    Returns NetworkX DiGraph.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    L, B, H, T, _ = attn_tensor.shape\n",
    "    attn_avg = attn_tensor.mean(dim=(0,1,2))  # (T, T)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for i, g1 in enumerate(gene_names):\n",
    "        for j, g2 in enumerate(gene_names):\n",
    "            if i != j and attn_avg[i, j] > threshold:\n",
    "                G.add_edge(g1, g2, weight=attn_avg[i, j].item())\n",
    "    return G\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_heatmap(attn_tensor, gene_names, layer=0, head=0, cell=0, top_n=30, threshold=None):\n",
    "    \"\"\"\n",
    "    Plot an attention heatmap from a specific layer/head/cell in the attention tensor.\n",
    "    \n",
    "    Args:\n",
    "        attn_tensor: Tensor of shape (L, B, H, T, T)\n",
    "        gene_names: List of gene names (length T)\n",
    "        layer: Layer index\n",
    "        head: Head index\n",
    "        cell: Cell index in batch\n",
    "        top_n: Only show top-N attended genes (by total attention)\n",
    "        threshold: Optional minimum attention to keep (set others to 0)\n",
    "    \"\"\"\n",
    "    A = attn_tensor[layer, cell, head].cpu().numpy()  # shape: (T, T)\n",
    "\n",
    "    # Rank genes by attention strength (incoming + outgoing)\n",
    "    gene_scores = A.sum(axis=0) + A.sum(axis=1)\n",
    "    top_indices = np.argsort(gene_scores)[-top_n:]\n",
    "\n",
    "    A_top = A[np.ix_(top_indices, top_indices)]\n",
    "    genes_top = [gene_names[i] for i in top_indices]\n",
    "\n",
    "    if threshold is not None:\n",
    "        A_top = np.where(A_top >= threshold, A_top, 0)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(A_top, xticklabels=genes_top, yticklabels=genes_top, cmap=\"viridis\", square=True)\n",
    "    plt.title(f\"Attention Map — Layer {layer}, Head {head}, Cell {cell}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation and visualization\n",
    "# UMAP/tSNE of cell embeddings\n",
    "# Visualize attention weights\n",
    "# Cluster attention maps\n",
    "# Compute cosine similarities of attention patterns across space\n",
    "\n",
    "B = 512  # batch size\n",
    "N = 10000  # total cells\n",
    "\n",
    "# Loop over mini-batches\n",
    "attn_all = []\n",
    "for i in range(0, N, B):\n",
    "    batch_expr = expr[i:i+B].unsqueeze(-1).to(device)\n",
    "    batch_gene_ids = torch.arange(expr.shape(1)).unsqueeze(0).repeat(batch_expr.size(0), 1).to(device)\n",
    "    attn = extract_all_attention(model, batch_gene_ids, batch_expr)  # (L, B, H, T, T)\n",
    "    attn_all.append(attn)\n",
    "\n",
    "# Concatenate over batch dim (dim=1)\n",
    "attn_tensor = torch.cat(attn_all, dim=1)  # (L, N, H, T, T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
