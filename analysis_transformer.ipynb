{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac12fe8e",
   "metadata": {},
   "source": [
    "# Transformer-based discovery of neuromodulatory co-expression modules\n",
    "\n",
    "This is a preliminary experiment to utilize the *attentional* component of transformer-based deep learning models to identify sets of neuromodulator-related genes that tend to be expressed in some contexts but not in others.\n",
    "\n",
    "Transformers, though *overkill*, are very good at learning higher-order interactions, especially context. The statement \"Gene B predicts Gene A but only near vasculature\" is something a transformer is good at learning. The output of this model is which genes *attend* to others across cells, creating gene-gene dependencies (\"attention maps\"). These are latent modules that are **conditional** on *space*, *region*, or other hidden cell states.\n",
    "\n",
    "Data:\n",
    "- ~200D vector per cell, + spatial coordinates & metadata (cell type, brain region)\n",
    "- Each *gene* is a token\n",
    "- *Positional encoding* based on spatial location\n",
    "\n",
    "### Model spec\n",
    "token_input = gene_id_embedding + f(expression_value) + cell_context_embedding, where the former is a learnable embedding of gene identity and the latter is a linear transformation of the expression value\n",
    "\n",
    "### Architecture\n",
    "Stack of transformer encoder blocks, where each block has a multi-head self-attention + feed-forward layer. Output -- one embedding per gene or cell-level aggregated embedding\n",
    "\n",
    "### Learning\n",
    "**Contrastive learning** encourages cells to learn from context, compressing them into a low-dimensional embedding space where contextual similarity is preserved. This encourages learning features of the data that discriminate spatial context.\n",
    "\n",
    "Help to learn cell-level representations\n",
    "\n",
    "### Interpretability\n",
    "Transformers provide *attention scores* which tell you something like, \"In this cell, gene A attends most to gene B and C.\" This can be used to identify\n",
    "context-specific gene-gene depencies, cluster genes into modules based on shared attention, or to visualize maps across space. We can also *cluster* to define\n",
    "co-expression modules.\n",
    "\n",
    "### Loss\n",
    "1. Masked gene modelling -- mask random 15% of genes and predict expression with MSE or Huber loss\n",
    "`L_mask = mean_squared_error(predicted, true_expression)`\n",
    "2. Contrastive loss -- InfoNCE between pairs of cells (negative pairs meaning random cells from distanct regions, or positive from same spatial bin or region)\n",
    "`L_contrastive = -log( exp(sim(z1, z2) / τ) / sum(exp(sim(z1, zk) / τ)) )`\n",
    "\n",
    "$L_{total} = \\lambda_{mask} L_{mask} + \\lambda_{contrastive} L_{contrastive}$\n",
    "\n",
    "### Additional tests\n",
    "- Use attention scores to see which genes are consistently “important” in which contexts\n",
    "- Perturb gene inputs (e.g., set one to zero) and see how predictions or attention change\n",
    "- Cluster cell embeddings to find distinct neuromodulatory states or microcircuit motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80070362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_gene_modules.py\n",
    "# Basic transformer with two heads -- masked gene prediction and contrastive learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import allendata\n",
    "import gc # memory handling\n",
    "\n",
    "class AttnTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, src, src_mask=None, is_causal=False, src_key_padding_mask=None):\n",
    "        src2, attn = self.self_attn(\n",
    "            src, src, src,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False,\n",
    "            is_causal=is_causal  # <- new\n",
    "        )\n",
    "        self.attn_weights = attn.detach()\n",
    "        return super().forward(\n",
    "            src, src_mask=src_mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "class GeneTransformer(nn.Module):\n",
    "    # Constructor and hyperparameters\n",
    "    # num_genes: number of unique genes in the dataset\n",
    "    # d_model: dimensionality of the embeddings\n",
    "    # nhead: number of attention heads\n",
    "    # num_layers: number of transformer encoder layers\n",
    "    # dim_feedforward: dimensionality of the feedforward network\n",
    "    # dropout: dropout rate for regularization\n",
    "    def __init__(self, \n",
    "                 num_genes=200,\n",
    "                 d_model=128,\n",
    "                 nhead=8,\n",
    "                 num_layers=4,\n",
    "                 dim_feedforward=256,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings -- each gene gets a learnable embedding vector of size d\n",
    "        self.gene_embedding = nn.Embedding(num_genes, d_model)\n",
    "        self.expr_embedding = nn.Linear(1, d_model) # Projects scalar gene expression value into same d_model space\n",
    "\n",
    "        # Positional encoding (optional, if using gene order)\n",
    "        #self.pos_embedding = nn.Parameter(torch.randn(1, num_genes, d_model))\n",
    "\n",
    "        # Transformer encoder first define the layer\n",
    "        encoder_layer = AttnTransformerEncoderLayer(\n",
    "            d_model=d_model * 2,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Then create the encoder itself\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Masked gene prediction head (token-level)\n",
    "        self.mask_head = nn.Linear(d_model * 2, 1) # takes in your vector and outputs a scalar for each gene\n",
    "\n",
    "        # Contrastive learning head (cell-level)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1) # pools per-gene outputs to create a single vector per cell\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model * 2)\n",
    "        ) # Non-linear projection for contrastive learning (SimCLR)\n",
    "\n",
    "    def forward(self, gene_ids, expr, mask=None, return_attention=False):\n",
    "        # Where B = batch size, T = number of genes, D = embedding dimension\n",
    "        # gene_ids: (B, T)\n",
    "        # expr: (B, T, 1)\n",
    "        x_id = self.gene_embedding(gene_ids)           # (B, T, D)\n",
    "        x_expr = self.expr_embedding(expr)             # (B, T, D)\n",
    "        # concatenate the embeddings instead of summing\n",
    "        x = torch.cat([x_id, x_expr], dim=-1)  # (B, T, 2D)\n",
    "\n",
    "        # Capture attention weights by running layers manually\n",
    "        if return_attention:\n",
    "            # Only get attention from the first layer for simplicity\n",
    "            layer = self.transformer.layers[0]\n",
    "            attn_module = layer.self_attn\n",
    "            attn_output, attn_weights = attn_module(x, x, x, need_weights=True, average_attn_weights=False)\n",
    "            return attn_weights  # shape: (B, num_heads, T, T)\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=mask)           # (B, T, 2D)\n",
    "\n",
    "        # Masked prediction output\n",
    "        mask_pred = self.mask_head(x).squeeze(-1)      # (B, T)\n",
    "\n",
    "        # Cell-level embedding for contrastive learning\n",
    "        pooled = self.pooler(x.transpose(1, 2)).squeeze(-1)  # (B, T, D) -> (B, D)\n",
    "        proj = self.projection_head(pooled)            # (B, D)\n",
    "\n",
    "        return mask_pred, proj\n",
    "\n",
    "\n",
    "def mask_gene_expressions(expr, mask_ratio=0.15):\n",
    "    \"\"\"\n",
    "    expr: Tensor of shape (B, T, 1)\n",
    "    Returns:\n",
    "        masked_expr: same shape, with some tokens zeroed\n",
    "        mask: Bool tensor of shape (B, T) where True = masked\n",
    "    \"\"\"\n",
    "    B, T, _ = expr.shape\n",
    "    mask = torch.rand(B, T) < mask_ratio\n",
    "    masked_expr = expr.clone()\n",
    "    masked_expr[mask.unsqueeze(-1)] = 0.0  # or NaN, or learned <MASK> token\n",
    "\n",
    "    return masked_expr, mask\n",
    "\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    \"\"\"\n",
    "    Compute MSE only on the masked gene positions.\n",
    "    \n",
    "    Args:\n",
    "        pred:   (B, T) model predictions\n",
    "        target: (B, T) ground truth expression values\n",
    "        mask:   (B, T) boolean tensor (True where loss should be computed)\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    masked_pred = pred[mask]\n",
    "    masked_target = target[mask]\n",
    "    return F.mse_loss(masked_pred, masked_target)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Start with T = 0.1, and then tune if:\n",
    "# \t•\tLoss isn’t decreasing well (try lowering T)\n",
    "# \t•\tModel isn’t distinguishing enough (try lowering T)\n",
    "# \t•\tModel collapses into tight clusters (try slightly increasing T)\n",
    "def contrastive_loss(embeddings, coords, temperature=0.1):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor of shape (B, D)\n",
    "    coords: Tensor of shape (B, 3), representing x, y, z positions of each cell\n",
    "    tau: scaling factor for the RBF kernel    \n",
    "    temperature: Scaling factor for similarity\n",
    "\n",
    "    Returns:\n",
    "        scalar contrastive loss (averaged over positive pairs)\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = F.normalize(embeddings, dim=1)  # (B, D)\n",
    "    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature # (B, B) ; cosine similarity\n",
    "    sim_matrix.fill_diagonal_(-1e9) # Avoid self-similarity\n",
    "\n",
    "    dists = (torch.cdist(coords, coords))**2  # (B, B)\n",
    "    tau = dists.mean()\n",
    "    pos_weights = torch.exp(-dists / tau)  # RBF-like affinity\n",
    "    pos_weights.fill_diagonal_(0)  # No self-positives\n",
    "    \n",
    "    log_probs = F.log_softmax(sim_matrix, dim=1) # Log softmax to get log probabilities\n",
    "    loss = -(pos_weights * log_probs).sum() / (pos_weights.sum() + 1e-8)\n",
    "\n",
    "    print(f\"Tau: {tau}\")\n",
    "    print(f\"Positive weights: {pos_weights.mean()}\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "def plot_embeddings(embeddings, title=\"Cell Embeddings (PCA)\"):\n",
    "    \"\"\"\n",
    "    embeddings: (N, D) tensor or numpy array of cell embeddings\n",
    "    \"\"\"\n",
    "    embeddings_np = embeddings.detach().cpu().numpy() if isinstance(embeddings, torch.Tensor) else embeddings\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_umap(embeddings, title=\"Cell Embeddings (UMAP)\"):\n",
    "    # Reduce to 2D using UMAP\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1)\n",
    "    reduced = reducer.fit_transform(embeddings.detach().cpu().numpy())\n",
    "\n",
    "    # Plot the result\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP 1\")\n",
    "    plt.ylabel(\"UMAP 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c061254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from disk...\n"
     ]
    }
   ],
   "source": [
    "goi = 'Drd|Chrn|Chrm|Htr|Adra|Adrb|Gri|Grm|Gab|Opr|Sigmar|Cnr|Glr|Hrh|Slc6a2|Slc6a4|Slc6a3'\n",
    "expression_full, gene, _, _, _ = allendata.load_preprocessed()\n",
    "expression_full = expression_full[expression_full['dataset'] == 'MERFISH']\n",
    "expression_full = expression_full.dropna(subset=['x_reconstructed', 'y_reconstructed', 'z_reconstructed'])\n",
    "\n",
    "# Separate data\n",
    "genelist = gene['gene_symbol'].tolist()\n",
    "expression = expression_full[genelist]\n",
    "metadata = expression_full[expression_full.columns.difference(genelist)]\n",
    "expression = expression.fillna(0)\n",
    "\n",
    "# Filter, subset, convert to tensor\n",
    "num_cells = 1000\n",
    "sample_idx = torch.randint(0, expression.shape[0], (num_cells,))\n",
    "expression = expression.iloc[sample_idx]\n",
    "expr = torch.tensor(expression.values, dtype=torch.float32)\n",
    "expr = expr.unsqueeze(-1)\n",
    "metadata = metadata.iloc[sample_idx]\n",
    "coords = metadata[['x_reconstructed','y_reconstructed','z_reconstructed']]\n",
    "coords = torch.tensor(coords.values, dtype=torch.float32)\n",
    "\n",
    "gene_ids = torch.arange(expression.shape[1]).unsqueeze(0).repeat(num_cells, 1)\n",
    "\n",
    "del goi, genelist, expression_full, metadata, expression\n",
    "gc.collect()\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "expr = expr.to(device)\n",
    "gene_ids = gene_ids.to(device)\n",
    "coords = coords.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a81e84a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau: 34.03376770019531\n",
      "Positive weights: 0.46037667989730835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:14<00:14, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau: 31.075395584106445\n",
      "Positive weights: 0.4570927917957306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:27<00:00, 13.89s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Masked Loss: 9.7772 | Contrastive Loss: 6.2534 | Total: 16.0307\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m batch_coords = coords[idx]\n\u001b[32m     35\u001b[39m masked_expr, mask = mask_gene_expressions(batch_expr, mask_ratio=\u001b[32m0.15\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m mask_pred, cell_embedding = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_gene_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m mloss = masked_mse_loss(mask_pred, batch_expr.squeeze(-\u001b[32m1\u001b[39m), mask)\n\u001b[32m     39\u001b[39m closs = contrastive_loss(cell_embedding, batch_coords)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mGeneTransformer.forward\u001b[39m\u001b[34m(self, gene_ids, expr, mask, return_attention)\u001b[39m\n\u001b[32m     97\u001b[39m     attn_output, attn_weights = attn_module(x, x, x, need_weights=\u001b[38;5;28;01mTrue\u001b[39;00m, average_attn_weights=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_weights  \u001b[38;5;66;03m# shape: (B, num_heads, T, T)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# (B, T, 2D)\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Masked prediction output\u001b[39;00m\n\u001b[32m    103\u001b[39m mask_pred = \u001b[38;5;28mself\u001b[39m.mask_head(x).squeeze(-\u001b[32m1\u001b[39m)      \u001b[38;5;66;03m# (B, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:517\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    514\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    525\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mAttnTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, is_causal, src_key_padding_mask)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask=\u001b[38;5;28;01mNone\u001b[39;00m, is_causal=\u001b[38;5;28;01mFalse\u001b[39;00m, src_key_padding_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     src2, attn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <- new\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mself\u001b[39m.attn_weights = attn.detach()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().forward(\n\u001b[32m     34\u001b[39m         src, src_mask=src_mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask\n\u001b[32m     35\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/activation.py:1373\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1347\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1348\u001b[39m         query,\n\u001b[32m   1349\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1370\u001b[39m         is_causal=is_causal,\n\u001b[32m   1371\u001b[39m     )\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:6378\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dropout_p > \u001b[32m0.0\u001b[39m:\n\u001b[32m   6376\u001b[39m     attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n\u001b[32m-> \u001b[39m\u001b[32m6378\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6380\u001b[39m attn_output = (\n\u001b[32m   6381\u001b[39m     attn_output.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous().view(tgt_len * bsz, embed_dim)\n\u001b[32m   6382\u001b[39m )\n\u001b[32m   6383\u001b[39m attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training config\n",
    "num_genes = expr.shape[1]\n",
    "d_model = 128\n",
    "batch_size = 512\n",
    "num_epochs = 5\n",
    "l_contrastive = 1.0\n",
    "l_mask = 1.0\n",
    "distance_threshold = 1\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = GeneTransformer(num_genes=num_genes, d_model=d_model).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss tracking\n",
    "loss_history = {\"total\": [], \"masked\": [], \"contrastive\": []}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    perm = torch.randperm(num_cells)\n",
    "    epoch_mloss = 0.0\n",
    "    epoch_closs = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for i in tqdm(range(0, num_cells, batch_size)):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        batch_gene_ids = gene_ids[idx]\n",
    "        batch_expr = expr[idx]\n",
    "        batch_coords = coords[idx]\n",
    "\n",
    "        masked_expr, mask = mask_gene_expressions(batch_expr, mask_ratio=0.15)\n",
    "        mask_pred, cell_embedding = model(batch_gene_ids, masked_expr)\n",
    "\n",
    "        mloss = masked_mse_loss(mask_pred, batch_expr.squeeze(-1), mask)\n",
    "        closs = contrastive_loss(cell_embedding, batch_coords)\n",
    "\n",
    "        loss = l_mask * mloss + l_contrastive * closs\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Update weights\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "        epoch_mloss += mloss.item()\n",
    "        epoch_closs += closs.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Record losses\n",
    "    mean_mloss = epoch_mloss / num_batches\n",
    "    mean_closs = epoch_closs / num_batches\n",
    "    mean_total = l_mask * mean_mloss + l_contrastive * mean_closs\n",
    "\n",
    "    loss_history[\"masked\"].append(mean_mloss)\n",
    "    loss_history[\"contrastive\"].append(mean_closs)\n",
    "    loss_history[\"total\"].append(mean_total)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"runs/gene_transformer_epoch{epoch+1}.pt\")\n",
    "\n",
    "    # Visualize embeddings\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_idx = torch.randint(0, num_cells, (1000,))\n",
    "        sample_gene_ids = gene_ids[sample_idx]\n",
    "        sample_expr = expr[sample_idx]\n",
    "        masked_expr, _ = mask_gene_expressions(sample_expr)\n",
    "        _, sample_embeddings = model(sample_gene_ids, masked_expr)\n",
    "        plot_embeddings(sample_embeddings, title=f\"Epoch {epoch+1} Cell Embeddings\")\n",
    "\n",
    "    # Record scalar losses\n",
    "    writer.add_scalar(\"Loss/Total\", mean_total, epoch)\n",
    "    writer.add_scalar(\"Loss/Masked\", mean_mloss, epoch)\n",
    "    writer.add_scalar(\"Loss/Contrastive\", mean_closs, epoch)\n",
    "\n",
    "    # Log sample embeddings (every N epochs)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        writer.add_embedding(\n",
    "            sample_embeddings,\n",
    "            tag=f\"Epoch_{epoch+1}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Masked Loss: {mean_mloss:.4f} | Contrastive Loss: {mean_closs:.4f} | Total: {mean_total:.4f}\")\n",
    "\n",
    "writer.flush()\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history[\"total\"], label=\"Total Loss\")\n",
    "plt.plot(loss_history[\"masked\"], label=\"Masked MSE\")\n",
    "plt.plot(loss_history[\"contrastive\"], label=\"Contrastive\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot UMAP\n",
    "# After model evaluation (for example, after getting sample_embeddings)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    sample_idx = torch.randint(0, num_cells, (1000,))\n",
    "    sample_gene_ids = gene_ids[sample_idx]\n",
    "    sample_expr = expr[sample_idx]\n",
    "    masked_expr, _ = mask_gene_expressions(sample_expr)\n",
    "    _, sample_embeddings = model(sample_gene_ids, masked_expr)\n",
    "    \n",
    "    # Visualize with UMAP\n",
    "    plot_umap(sample_embeddings, title=f\"Epoch {epoch+1} Cell Embeddings (UMAP)\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_attention(model, gene_ids, expr, return_tensor=True):\n",
    "    \"\"\"\n",
    "    Extracts attention maps from all layers and heads.\n",
    "    Args:\n",
    "        model: Trained GeneTransformer\n",
    "        gene_ids: (B, T) tensor\n",
    "        expr: (B, T, 1) tensor\n",
    "        return_tensor: If True, returns a tensor (L, B, H, T, T), else returns list of Tensors\n",
    "    Returns:\n",
    "        Tensor of shape (L, B, H, T, T)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(gene_ids, expr)\n",
    "\n",
    "    # Collect attention from each layer\n",
    "    attn_all = []\n",
    "    for layer in model.transformer.layers:\n",
    "        attn = layer.attn_weights  # (B, H, T, T)\n",
    "        attn_all.append(attn.unsqueeze(0))  # (1, B, H, T, T)\n",
    "\n",
    "    if return_tensor:\n",
    "        return torch.cat(attn_all, dim=0)  # (L, B, H, T, T)\n",
    "    else:\n",
    "        return attn_all\n",
    "    \n",
    "def build_multilayer_gene_network(attn_tensor, gene_names, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Create a gene attention network from (L, B, H, T, T) attention tensor.\n",
    "    Returns NetworkX DiGraph.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    L, B, H, T, _ = attn_tensor.shape\n",
    "    attn_avg = attn_tensor.mean(dim=(0,1,2))  # (T, T)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for i, g1 in enumerate(gene_names):\n",
    "        for j, g2 in enumerate(gene_names):\n",
    "            if i != j and attn_avg[i, j] > threshold:\n",
    "                G.add_edge(g1, g2, weight=attn_avg[i, j].item())\n",
    "    return G\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_heatmap(attn_tensor, gene_names, layer=0, head=0, cell=0, top_n=30, threshold=None):\n",
    "    \"\"\"\n",
    "    Plot an attention heatmap from a specific layer/head/cell in the attention tensor.\n",
    "    \n",
    "    Args:\n",
    "        attn_tensor: Tensor of shape (L, B, H, T, T)\n",
    "        gene_names: List of gene names (length T)\n",
    "        layer: Layer index\n",
    "        head: Head index\n",
    "        cell: Cell index in batch\n",
    "        top_n: Only show top-N attended genes (by total attention)\n",
    "        threshold: Optional minimum attention to keep (set others to 0)\n",
    "    \"\"\"\n",
    "    A = attn_tensor[layer, cell, head].cpu().numpy()  # shape: (T, T)\n",
    "\n",
    "    # Rank genes by attention strength (incoming + outgoing)\n",
    "    gene_scores = A.sum(axis=0) + A.sum(axis=1)\n",
    "    top_indices = np.argsort(gene_scores)[-top_n:]\n",
    "\n",
    "    A_top = A[np.ix_(top_indices, top_indices)]\n",
    "    genes_top = [gene_names[i] for i in top_indices]\n",
    "\n",
    "    if threshold is not None:\n",
    "        A_top = np.where(A_top >= threshold, A_top, 0)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(A_top, xticklabels=genes_top, yticklabels=genes_top, cmap=\"viridis\", square=True)\n",
    "    plt.title(f\"Attention Map — Layer {layer}, Head {head}, Cell {cell}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation and visualization\n",
    "# UMAP/tSNE of cell embeddings\n",
    "# Visualize attention weights\n",
    "# Cluster attention maps\n",
    "# Compute cosine similarities of attention patterns across space\n",
    "\n",
    "B = 512  # batch size\n",
    "N = 10000  # total cells\n",
    "\n",
    "# Loop over mini-batches\n",
    "attn_all = []\n",
    "for i in range(0, N, B):\n",
    "    batch_expr = expr[i:i+B].unsqueeze(-1).to(device)\n",
    "    batch_gene_ids = torch.arange(expr.shape(1)).unsqueeze(0).repeat(batch_expr.size(0), 1).to(device)\n",
    "    attn = extract_all_attention(model, batch_gene_ids, batch_expr)  # (L, B, H, T, T)\n",
    "    attn_all.append(attn)\n",
    "\n",
    "# Concatenate over batch dim (dim=1)\n",
    "attn_tensor = torch.cat(attn_all, dim=1)  # (L, N, H, T, T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
